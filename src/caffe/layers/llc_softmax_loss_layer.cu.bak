/*
 * llc_softmax_loss_layer.cu
 *
 *  Created on: Feb 25, 2014
 *      Author: jieshen
 */

#include <algorithm>
#include <cfloat>
#include <vector>

#include "caffe/layer.hpp"
#include "caffe/vision_layers.hpp"
#include "caffe/util/math_functions.hpp"

using std::max;

namespace caffe
{

  // bottom[0]: data
  // bottom[1]: label
  // bottom[2]: LLC code
  // refer to the LLCDataLayer implementation
  template<typename Dtype>
  void LLCSoftmaxWithLossLayer<Dtype>::SetUp(const vector<Blob<Dtype>*>& bottom,
                                             vector<Blob<Dtype>*>* top)
  {
    CHECK_EQ(bottom.size(), 3)<< "LLCSoftmaxLoss Layer takes label blob and LLC code blob as input.";
    CHECK_EQ(top->size(), 0) << "LLCSoftmaxLoss Layer takes no blob as output.";
    llc_softmax_bottom_vec_.clear();
    llc_softmax_bottom_vec_.push_back(bottom[0]);
    llc_softmax_top_vec_.push_back(&prob_);
    llc_softmax_layer_->SetUp(llc_softmax_bottom_vec_, &llc_softmax_top_vec_);
  }

  template<typename Dtype>
  void LLCSoftmaxWithLossLayer<Dtype>::Forward_cpu(
      const vector<Blob<Dtype>*>& bottom, vector<Blob<Dtype>*>* top)
  {
    // The forward pass computes the softmax prob values.
    llc_softmax_bottom_vec_[0] = bottom[0];
    llc_softmax_layer_->Forward(llc_softmax_bottom_vec_, &llc_softmax_top_vec_);
  }

  template<typename Dtype>
  void LLCSoftmaxWithLossLayer<Dtype>::Forward_gpu(
      const vector<Blob<Dtype>*>& bottom, vector<Blob<Dtype>*>* top)
  {
    // The forward pass computes the softmax prob values.
    llc_softmax_bottom_vec_[0] = bottom[0];
    llc_softmax_layer_->Forward(llc_softmax_bottom_vec_, &llc_softmax_top_vec_);
  }

  // TODO change to our backward algorithm
  template<typename Dtype>
  Dtype LLCSoftmaxWithLossLayer<Dtype>::Backward_cpu(
      const vector<Blob<Dtype>*>& top, const bool propagate_down,
      vector<Blob<Dtype>*>* bottom)
  {
    // First, compute the diff
    Dtype* bottom_diff = (*bottom)[0]->mutable_cpu_diff();
    const Dtype* prob_data = prob_.cpu_data();
    memcpy(bottom_diff, prob_data, sizeof(Dtype) * prob_.count());
    const Dtype* label = (*bottom)[1]->cpu_data();
    int num = prob_.num();
    int dim = prob_.count() / num;
    Dtype loss = 0;
    for (int i = 0; i < num; ++i)
    {
      bottom_diff[i * dim + static_cast<int>(label[i])] -= 1;
      loss += -log(
          max(prob_data[i * dim + static_cast<int>(label[i])], FLT_MIN));
    }
    // Scale down gradient
    caffe_scal(prob_.count(), Dtype(1) / num, bottom_diff);
    return loss / num;
  }

  template<typename Dtype>
  Dtype LLCSoftmaxWithLossLayer<Dtype>::Backward_gpu(
      const vector<Blob<Dtype>*>& top, const bool propagate_down,
      vector<Blob<Dtype>*>* bottom)
  {
    // TODO(Yangqing): implement the GPU version of softmax.
    return Backward_cpu(top, propagate_down, bottom);
  }

  INSTANTIATE_CLASS(LLCSoftmaxWithLossLayer);

}  // namespace caffe

